{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11471894,"sourceType":"datasetVersion","datasetId":7189396}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom collections import deque\nimport random\nimport time\nimport wandb\nimport logging \nfrom tqdm.notebook import tqdm # Use notebook version for Kaggle UI\nimport argparse # Keep for str2bool if used by helpers\nimport copy # Needed to save best model state\nfrom wandb.sdk.wandb_settings import Settings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:18:23.324204Z","iopub.execute_input":"2025-04-19T20:18:23.325019Z","iopub.status.idle":"2025-04-19T20:18:23.330190Z","shell.execute_reply.started":"2025-04-19T20:18:23.324991Z","shell.execute_reply":"2025-04-19T20:18:23.329687Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n    wandb.login(key=wandb_api_key)\n    print(\"W&B login successful using Kaggle Secret.\")\nexcept ImportError:\n    print(\"kaggle_secrets not found. Ensure it's enabled or use interactive/env var login.\")\n    wandb.login() # Will use env var WANDB_API_KEY if set, otherwise prompt\nexcept Exception as e:\n     print(f\"W&B login using Kaggle Secret failed: {e}. Trying other methods.\")\n     wandb.login() # Fallback attempt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/inaturalist-12k/inaturalist_12K\"\nSEED = 42\nIMG_SIZE = 224\nNUM_WORKERS = 2 # Kaggle typical limit\nVAL_SPLIT = 0.2\nWANDB_PROJECT_NAME = \"CNN_FROM_SCRATCH_SWEEP\" # Project for this specific run\nWANDB_ENTITY = None # Optional: Let wandb infer or set \"user_or_team_name\"\nOUTPUT_DIR = \"/kaggle/working/output_best_run_q4\" # Save outputs here in Kaggle\nMODEL_SAVE_NAME = \"best_cnn_q4_model.pth\" # Filename for the saved model\n\n# --- <<< EDIT THESE HYPERPARAMETERS FOR YOUR BEST RUN >>> ---\nBEST_CONFIG = {\n    'num_filters': 64,\n    'filter_size': 3,\n    'filter_organization': 'same',\n    'activation': 'mish',\n    'dense_neurons': 256,\n    'dropout_rate': 0.35,\n    'batch_norm': True,\n    'data_augmentation': True,\n    'learning_rate': 0.0005,\n    'weight_decay': 0.0005,\n    'batch_size': 32,\n    'epochs': 15, # Number of epochs for THIS run\n}\n# --- End of Hyperparameters ---\n\n# --- Grid/Visualization Settings ---\nGRID_ROWS = 10\nGRID_COLS = 3\n\n# --- Setup Logging ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s', stream=sys.stdout)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    \"\"\"Sets seed for reproducibility.\"\"\"\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    logging.debug(f\"Seed set to {seed}\")\n\ndef seed_worker(worker_id):\n    \"\"\"Seeds DataLoader workers.\"\"\"\n    worker_seed = (torch.initial_seed()) % 2**32\n    np.random.seed(worker_seed); random.seed(worker_seed)\n    logging.debug(f\"Worker {worker_id} seeded with {worker_seed}\")\n\ng_dataloader_seed = torch.Generator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:18:26.744039Z","iopub.execute_input":"2025-04-19T20:18:26.744316Z","iopub.status.idle":"2025-04-19T20:18:26.749708Z","shell.execute_reply.started":"2025-04-19T20:18:26.744293Z","shell.execute_reply":"2025-04-19T20:18:26.748859Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class INaturalistDataset(Dataset):\n    \"\"\" Loads iNaturalist subset images. \"\"\"\n    def __init__(self, root_dir, transform=None):\n        if not os.path.isdir(root_dir): raise FileNotFoundError(f\"Dir not found: {root_dir}\")\n        self.root_dir=root_dir; self.transform=transform\n        try: self.classes=sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))]); assert self.classes\n        except: raise ValueError(f\"No class subdirs found: {root_dir}\")\n        self.class_to_idx={c:i for i,c in enumerate(self.classes)}; self.idx_to_class={i:c for c,i in self.class_to_idx.items()}\n        self.images, self.labels = [], []\n        img_count = 0\n        for cn in self.classes:\n            cd=os.path.join(root_dir,cn)\n            try:\n                for imn in os.listdir(cd):\n                    imp=os.path.join(cd,imn)\n                    if os.path.isfile(imp) and imn.lower().endswith(('.png','.jpg','.jpeg','.gif','.bmp')): self.images.append(imp); self.labels.append(self.class_to_idx[cn]); img_count+=1\n            except OSError as e: logging.warning(f\"Read err {cd}:{e}\")\n        if not self.images: raise RuntimeError(f\"No images: {root_dir}\")\n        logging.info(f\"Dataset({os.path.basename(root_dir)}): {len(self.classes)} classes, {img_count} images.\")\n    def __len__(self): return len(self.images)\n    def __getitem__(self, idx):\n        imp=self.images[idx]; lbl=self.labels[idx]\n        try:\n            with Image.open(imp) as img: image=img.convert(\"RGB\")\n        except Exception as e: logging.error(f\"Err IMG {imp}: {e}\"); return torch.zeros((3,IMG_SIZE,IMG_SIZE),dtype=torch.float32),torch.tensor(-1,dtype=torch.long)\n        if self.transform: image=self.transform(image)\n        return image, torch.tensor(lbl, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:18:29.342258Z","iopub.execute_input":"2025-04-19T20:18:29.342529Z","iopub.status.idle":"2025-04-19T20:18:29.351875Z","shell.execute_reply.started":"2025-04-19T20:18:29.342508Z","shell.execute_reply":"2025-04-19T20:18:29.351194Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def get_data_loaders(data_dir, batch_size=32, val_split=0.2, augment=True, num_workers=2, img_size=224, seed=42, generator=None):\n    \"\"\" Creates DataLoaders without sklearn, with deterministic worker seeding. \"\"\"\n    logging.info(f\"DataLoaders: batch={batch_size}, augment={augment}, val_split={val_split}, workers={num_workers}\")\n    if generator is None: generator = torch.Generator().manual_seed(seed)\n    normalize=transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]); target_size=(img_size,img_size)\n    if augment: train_transform=transforms.Compose([transforms.Resize(target_size),transforms.RandomHorizontalFlip(),transforms.RandomRotation(15),transforms.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.1,hue=0.1),transforms.ToTensor(),normalize])\n    else: train_transform=transforms.Compose([transforms.Resize(target_size),transforms.ToTensor(),normalize])\n    val_test_transform=transforms.Compose([transforms.Resize(target_size),transforms.ToTensor(),normalize])\n    train_dir,test_dir=os.path.join(data_dir,'train'),os.path.join(data_dir,'val'); assert os.path.isdir(train_dir),f\"No train dir: {train_dir}\"; assert os.path.isdir(test_dir),f\"No test dir: {test_dir}\"\n    try: full_dataset=INaturalistDataset(root_dir=train_dir,transform=train_transform); test_dataset=INaturalistDataset(root_dir=test_dir,transform=val_test_transform)\n    except Exception as e: logging.error(f\"Dataset load error: {e}\"); raise e\n    targets=np.array(full_dataset.labels); dataset_size=len(targets); train_indices=[]; val_indices=[]; val_loader=None; num_classes=len(full_dataset.classes); valid_indices=[idx for idx,l in enumerate(targets) if l>=0]\n    if 0<val_split<1 and len(valid_indices)>=2 and num_classes>0:\n        local_rng=np.random.RandomState(seed); indices_by_class={lbl:[] for lbl in range(num_classes)}\n        for idx in valid_indices: indices_by_class[targets[idx]].append(idx)\n        for label,indices in indices_by_class.items():\n            n_cls=len(indices);\n            if n_cls==0: continue\n            local_rng.shuffle(indices); n_val=int(np.floor(val_split*n_cls))\n            if n_cls>1 and n_val==n_cls: n_val=n_cls-1\n            elif n_cls<=1 and val_split>0: n_val=0\n            val_indices.extend(indices[:n_val]); train_indices.extend(indices[n_val:])\n        if not train_indices or not val_indices:\n            logging.warning(\"Strat split failed. Random split.\"); local_rng.shuffle(valid_indices); split_point=int(len(valid_indices)*(1-val_split)); train_indices=valid_indices[:split_point]; val_indices=valid_indices[split_point:]\n        logging.info(f\"Split (seed {seed}): {len(train_indices)} train, {len(val_indices)} val.\")\n        local_rng.shuffle(train_indices)\n        original_transform=full_dataset.transform; full_dataset.transform=val_test_transform\n        val_subset=Subset(full_dataset,val_indices); full_dataset.transform=original_transform\n        val_loader=DataLoader(val_subset,batch_size=batch_size,shuffle=False,num_workers=num_workers,pin_memory=torch.cuda.is_available(),worker_init_fn=seed_worker if num_workers>0 else None,generator=generator if num_workers>0 else None)\n    else: logging.warning(\"Val split skipped.\"); train_indices=valid_indices\n    train_subset=Subset(full_dataset,train_indices)\n    use_persistent_workers=num_workers>0 and sys.version_info>=(3,8); loader_kwargs={'persistent_workers':True,'prefetch_factor':2} if use_persistent_workers else {}\n    train_loader=DataLoader(train_subset,batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=torch.cuda.is_available(),drop_last=True,worker_init_fn=seed_worker if num_workers>0 else None,generator=generator if num_workers>0 else None,**loader_kwargs)\n    test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=num_workers,pin_memory=torch.cuda.is_available(),worker_init_fn=seed_worker if num_workers>0 else None,generator=generator if num_workers>0 else None,**loader_kwargs)\n    return train_loader, val_loader, test_loader, full_dataset.classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:19:54.845066Z","iopub.execute_input":"2025-04-19T20:19:54.845612Z","iopub.status.idle":"2025-04-19T20:19:54.857833Z","shell.execute_reply.started":"2025-04-19T20:19:54.845588Z","shell.execute_reply":"2025-04-19T20:19:54.857010Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"_ACTIVATIONS = {'relu': nn.ReLU, 'gelu': nn.GELU, 'silu': nn.SiLU, 'mish': nn.Mish}\nclass CustomCNN(nn.Module):\n    # (Same implementation as previous versions)\n    def __init__(self, input_channels=3, num_classes=10, filter_sizes=None, num_filters=None, filter_organization=\"same\", activation_name='relu', dense_neurons=128, dropout_rate=0.3, batch_norm=True, img_size=224):\n        super(CustomCNN, self).__init__(); # ... (rest of __init__ - same as before) ...\n        if filter_sizes is None: filter_sizes=[3]*5\n        elif len(filter_sizes)!=5: raise ValueError(\"filter_sizes must be list[5]\")\n        if num_filters is None: num_filters_list=[32]*5\n        elif isinstance(num_filters,int): base=num_filters; org=filter_organization; nl=[max(1,base*(2**i) if org==\"double\" else base//(2**i) if org==\"half\" else base) for i in range(5)]; num_filters_list=nl\n        elif isinstance(num_filters,list) and len(num_filters)==5: num_filters_list=[max(1,f) for f in num_filters]\n        else: raise ValueError(\"num_filters must be int or list[5]\")\n        if activation_name not in _ACTIVATIONS: raise ValueError(f\"Unsupported activation: {activation_name}\")\n        activation=_ACTIVATIONS[activation_name]; self.layers=nn.ModuleList(); cin=input_channels; cdim=img_size\n        for i in range(5):\n            ks,cout,p=filter_sizes[i],num_filters_list[i],filter_sizes[i]//2; conv=nn.Conv2d(cin,cout,ks,padding=p,bias=not batch_norm); self.layers.append(conv)\n            if batch_norm: self.layers.append(nn.BatchNorm2d(cout))\n            self.layers.append(activation()); self.layers.append(nn.MaxPool2d(2,2)); cdim//=2\n            if dropout_rate>0: self.layers.append(nn.Dropout(dropout_rate))\n            cin=cout\n        self.flat_size=num_filters_list[-1]*cdim*cdim; assert self.flat_size>0, f\"Flat size <=0 for img {img_size}\"\n        self.fc1=nn.Linear(self.flat_size,dense_neurons); self.do_fc=nn.Dropout(dropout_rate); self.fc2=nn.Linear(dense_neurons,num_classes)\n        self.config={'nf':num_filters_list,'fs':filter_sizes,'act':activation_name,'dn':dense_neurons,'dr':dropout_rate,'bn':batch_norm,'isz':img_size,'nc':num_classes}\n    def forward(self,x):\n        for layer in self.layers: x=layer(x)\n        x=x.view(x.size(0),-1); x=F.relu(self.fc1(x)); x=self.do_fc(x); x=self.fc2(x)\n        return x\n    def count_parameters(self): return sum(p.numel() for p in self.parameters() if p.requires_grad)\n    def calculate_computations(self): # Simplified FLOPs estimate\n        # (Same calculation logic as before)\n        macs=0; h=w=self.config['isz']; cin=3; nf=self.config['nf']; fs=self.config['fs']; ch,cw=h,w; nc=self.config.get('nc', 10); dn=self.config.get('dn', 128)\n        for i in range(5): k,cout=fs[i],nf[i]; macs+=k*k*cin*cout*ch*cw; macs+=2*cout*ch*cw if self.config['bn'] else 0; macs+=cout*ch*cw; ch//=2; cw//=2; cin=cout\n        flat=nf[-1]*ch*cw; macs+=flat*dn; macs+=dn; macs+=dn*nc; return 2*macs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:18:39.703018Z","iopub.execute_input":"2025-04-19T20:18:39.703541Z","iopub.status.idle":"2025-04-19T20:18:39.715584Z","shell.execute_reply.started":"2025-04-19T20:18:39.703494Z","shell.execute_reply":"2025-04-19T20:18:39.715022Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"_MEAN = np.array([0.485, 0.456, 0.406])\n_STD = np.array([0.229, 0.224, 0.225])\ndef denormalize(tensor):\n    \"\"\"Denormalizes a tensor image for display.\"\"\"\n    try:\n        tensor=tensor.clone().cpu().numpy(); tensor=np.transpose(tensor,(1,2,0))\n        tensor=_STD*tensor+_MEAN; tensor=np.clip(tensor,0,1)\n        return tensor\n    except Exception as e: logging.error(f\"Dnorm err:{e}\"); return np.zeros((IMG_SIZE,IMG_SIZE,3))\n\n# =============================================================================\n# Main Training & Evaluation Function\n# =============================================================================\ndef train_and_evaluate(config, fixed_args):\n    \"\"\" Trains, evaluates, generates grid using the provided config. \"\"\"\n    run = None\n    best_model_state = None\n    best_val_acc = 0.0\n    best_epoch = 0\n    final_test_acc = 0.0\n\n    try:\n        # --- W&B Init ---\n        # Generate a descriptive run name\n        run_name = (f\"f{config['num_filters']}_k{config['filter_size']}_d{config['dense_neurons']}_bs{config['batch_size']}\"\n                    f\"_{config['activation'][:3]}_{config['filter_organization'][:3]}\"\n                    f\"_lr{config['learning_rate']:.1E}_do{config['dropout_rate']:.1f}\"\n                    f\"_bn{str(config['batch_norm'])[0]}_aug{str(config['data_augmentation'])[0]}\")\n        run_name = run_name.replace('.','p').replace('-','').replace('E','e')\n\n        run = wandb.init(\n            project=fixed_args['wandb_project'],\n            entity=fixed_args['wandb_entity'],\n            config=config, # Log the hyperparameters used for this run\n            job_type=\"train_evaluate_config\", # Set job type\n            name=run_name[:128], # Use generated name, limit length\n            settings=Settings(init_timeout=300)\n        )\n        if not run: raise Exception(\"wandb.init failed\")\n        logging.info(f\"--- Training & Evaluating Config ---\")\n        logging.info(f\"W&B Run: {wandb.run.name} ({run.id})\")\n        logging.info(f\"Config: {dict(wandb.config)}\") # Log the config used\n\n        # --- Setup ---\n        set_seed(fixed_args['seed'])\n        g_dataloader_seed.manual_seed(fixed_args['seed']) # Seed the DataLoader generator\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        logging.info(f\"Using device: {device}\")\n\n        # --- Data Loaders ---\n        train_loader, val_loader, test_loader, classes = get_data_loaders(\n            data_dir=fixed_args['data_dir'], batch_size=config['batch_size'], augment=config['data_augmentation'],\n            num_workers=fixed_args['num_workers'], img_size=fixed_args['img_size'],\n            val_split=fixed_args['val_split'], seed=fixed_args['seed'],\n            generator=g_dataloader_seed\n        )\n        num_classes = len(classes)\n        idx_to_class = {i: name for i, name in enumerate(classes)}\n\n        # --- Model, Loss, Optimizer, Scheduler ---\n        model = CustomCNN(\n            num_classes=num_classes, num_filters=config['num_filters'],\n            filter_organization=config['filter_organization'], filter_sizes=[config['filter_size']] * 5,\n            activation_name=config['activation'], dense_neurons=config['dense_neurons'],\n            dropout_rate=config['dropout_rate'], batch_norm=config['batch_norm'], img_size=fixed_args['img_size']\n        ).to(device)\n\n        wandb.watch(model, log=\"all\", log_freq=100)\n        total_params = model.count_parameters(); total_flops = model.calculate_computations()\n        wandb.log({\"total_parameters\": total_params, \"total_flops_estimate\": total_flops}, commit=False)\n        logging.info(f\"Model Params: {total_params:,}, Est. FLOPs: {total_flops:,.0f}\")\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config.get('weight_decay', 0))\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.2, verbose=False)\n\n        # --- Training & Validation Loop ---\n        epochs_to_run = config['epochs']\n        logging.info(f\"Starting training for {epochs_to_run} epochs...\")\n\n        for epoch in range(epochs_to_run):\n            epoch_start_time = time.time()\n            # (Training Epoch Logic)\n            model.train(); running_loss = 0.0; correct_train = 0; total_train = 0\n            train_pbar = tqdm(train_loader, desc=f\"Ep {epoch+1} Train\", leave=False, file=sys.stdout)\n            for i, batch_data in enumerate(train_pbar):\n                 try: inputs, labels = batch_data; assert not torch.any(labels < 0)\n                 except: logging.warning(f\"Skip bad train batch {i}\"); continue\n                 inputs, labels = inputs.to(device), labels.to(device)\n                 try:\n                    optimizer.zero_grad(set_to_none=True); outputs = model(inputs)\n                    loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n                    running_loss += loss.item(); _, predicted = outputs.max(1)\n                    total_train += labels.size(0); correct_train += predicted.eq(labels).sum().item()\n                    if i % 20 == 0 and total_train > 0: train_pbar.set_postfix({'L': f'{running_loss/(i+1):.3f}', 'Acc': f'{100.*correct_train/total_train:.1f}%'})\n                 except RuntimeError as e: logging.error(f\"Runtime error: {e}\"); torch.cuda.empty_cache(); raise e\n            train_pbar.close()\n            epoch_train_loss = running_loss/len(train_loader) if len(train_loader)>0 else 0\n            epoch_train_acc = 100.*correct_train/total_train if total_train>0 else 0\n\n            # (Validation Epoch Logic)\n            epoch_val_loss = float('nan'); epoch_val_acc = float('nan')\n            if val_loader and len(val_loader) > 0:\n                model.eval(); val_correct = 0; val_total = 0; val_loss_accum = 0.0\n                val_pbar = tqdm(val_loader, desc=f\"Ep {epoch+1} Val \", leave=False, file=sys.stdout)\n                with torch.no_grad():\n                    for i_val, batch_data in enumerate(val_pbar):\n                        try: inputs, labels = batch_data; assert not torch.any(labels < 0)\n                        except: logging.warning(f\"Skip bad val batch {i_val}\"); continue\n                        inputs, labels = inputs.to(device), labels.to(device)\n                        try:\n                            outputs = model(inputs); loss = criterion(outputs, labels)\n                            val_loss_accum += loss.item() * inputs.size(0)\n                            _, predicted = outputs.max(1)\n                            val_total += labels.size(0); val_correct += predicted.eq(labels).sum().item()\n                            if i_val % 10 == 0 and val_total > 0: val_pbar.set_postfix({'L': f'{val_loss_accum/val_total:.3f}', 'Acc': f'{100.*val_correct/val_total:.1f}%'})\n                        except RuntimeError as e: logging.error(f\"Val forward error: {e}\")\n                val_pbar.close()\n                if val_total > 0:\n                    epoch_val_loss = val_loss_accum / val_total\n                    epoch_val_acc = 100. * val_correct / val_total\n                    scheduler.step(epoch_val_loss)\n\n            # --- Logging ---\n            epoch_duration = time.time() - epoch_start_time\n            log_dict = {'epoch': epoch+1, 'train_loss': epoch_train_loss, 'train_accuracy': epoch_train_acc, 'lr': optimizer.param_groups[0]['lr'], 'epoch_sec': epoch_duration}\n            if not np.isnan(epoch_val_loss): log_dict['val_loss'] = epoch_val_loss\n            if not np.isnan(epoch_val_acc): log_dict['val_accuracy'] = epoch_val_acc\n            wandb.log(log_dict, commit=True)\n            logging.info(f'E{epoch+1}/{epochs_to_run}|Tr L:{epoch_train_loss:.3f},Tr Acc:{epoch_train_acc:.2f}%|'+(f'Val L:{epoch_val_loss:.3f},Val Acc:{epoch_val_acc:.2f}%|' if not np.isnan(epoch_val_acc) else 'Val:N/A|')+f'LR:{optimizer.param_groups[0][\"lr\"]:.1E}|T:{epoch_duration:.1f}s')\n\n            # --- Save Best Model State IN MEMORY ---\n            if not np.isnan(epoch_val_acc) and epoch_val_acc > best_val_acc:\n                best_val_acc = epoch_val_acc; best_epoch = epoch + 1\n                best_model_state = copy.deepcopy(model.state_dict())\n                logging.info(f\"*** Best val acc: {best_val_acc:.2f}% at Ep {best_epoch} ***\")\n\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n        logging.info(f\"--- Training Finished. Best Val Acc: {best_val_acc:.2f}% at epoch {best_epoch} ---\")\n\n        # --- Load Best Model State for Evaluation ---\n        if best_model_state:\n            model.load_state_dict(best_model_state)\n            logging.info(f\"Loaded best model state (from epoch {best_epoch}) for evaluation.\")\n        else:\n            logging.warning(\"No validation improvement or no validation done. Evaluating using final epoch state.\")\n\n        # --- Evaluation on Test Set ---\n        model.eval(); correct_test = 0; total_test = 0; test_loss_accum = 0.0\n        all_test_images = []; all_test_labels = []; all_test_preds = []\n        logging.info(\"Starting evaluation on the test set...\")\n        test_pbar = tqdm(test_loader, desc=\"Testing\", leave=False, file=sys.stdout)\n        criterion_test = nn.CrossEntropyLoss()\n\n        with torch.no_grad():\n            for inputs, labels in test_pbar:\n                 if torch.any(labels < 0): continue\n                 inputs, labels = inputs.to(device), labels.to(device)\n                 outputs = model(inputs); loss = criterion_test(outputs, labels)\n                 test_loss_accum += loss.item() * inputs.size(0)\n                 _, predicted = outputs.max(1)\n                 total_test += labels.size(0); correct_test += (predicted == labels).sum().item()\n                 if len(all_test_images) < fixed_args['grid_rows'] * fixed_args['grid_cols'] * 2: # Use fixed_args here\n                      all_test_images.append(inputs.cpu()); all_test_labels.append(labels.cpu()); all_test_preds.append(predicted.cpu())\n        test_pbar.close()\n\n        final_test_loss = test_loss_accum / total_test if total_test > 0 else float('nan')\n        final_test_acc = 100. * correct_test / total_test if total_test > 0 else 0\n        logging.info(f'Final Test Loss: {final_test_loss:.4f}, Final Test Accuracy: {final_test_acc:.2f}%')\n\n        # --- Log Final Test Metrics to W&B Summary ---\n        wandb.run.summary[\"best_val_accuracy\"] = best_val_acc\n        wandb.run.summary[\"best_epoch\"] = best_epoch\n        wandb.run.summary[\"final_test_loss\"] = final_test_loss\n        wandb.run.summary[\"final_test_accuracy\"] = final_test_acc # Key Q4 metric\n\n        # --- Generate and Log Prediction Grid ---\n        # ... (Prediction grid generation logic - needs fixed_args) ...\n        logging.info(\"Generating prediction grid...\")\n        try:\n            if all_test_images:\n                all_test_images=torch.cat(all_test_images); all_test_labels=torch.cat(all_test_labels); all_test_preds=torch.cat(all_test_preds)\n                num_classes_plot=min(num_classes, fixed_args['grid_rows']); samples_per_class=fixed_args['grid_cols']\n                plt.figure(figsize=(samples_per_class*3, num_classes_plot*3.5)); plotted_count=0\n                plot_indices_master = torch.randperm(len(all_test_images), generator=torch.Generator().manual_seed(fixed_args['seed'])) # Seeded shuffle\n                class_plotted_counts = {i: 0 for i in range(num_classes)}\n                for idx in plot_indices_master:\n                    if plotted_count >= num_classes_plot*samples_per_class: break\n                    true_lbl_idx = all_test_labels[idx].item();\n                    if true_lbl_idx < 0 or true_lbl_idx >= num_classes: continue # Skip invalid labels\n                    if class_plotted_counts[true_lbl_idx] < samples_per_class:\n                        img=all_test_images[idx]; pred_lbl_idx=all_test_preds[idx].item()\n                        img_denorm=denormalize(img)\n                        ax_row = true_lbl_idx % num_classes_plot; ax_col = class_plotted_counts[true_lbl_idx]\n                        plot_idx = ax_row * samples_per_class + ax_col + 1\n                        if plot_idx <= num_classes_plot * samples_per_class:\n                            ax=plt.subplot(num_classes_plot, samples_per_class, plot_idx)\n                            ax.imshow(img_denorm); title_color = 'green' if true_lbl_idx==pred_lbl_idx else 'red'\n                            ax.set_title(f'T:{idx_to_class.get(true_lbl_idx,\"?\")}\\nP:{idx_to_class.get(pred_lbl_idx,\"?\")}',color=title_color, fontsize=9); ax.axis('off')\n                            class_plotted_counts[true_lbl_idx] += 1; plotted_count += 1\n                plt.tight_layout(); grid_save_path = os.path.join(fixed_args['output_dir'], 'prediction_grid_best.png')\n                os.makedirs(fixed_args['output_dir'], exist_ok=True); plt.savefig(grid_save_path, dpi=150); plt.close()\n                wandb.log({\"prediction_grid\": wandb.Image(grid_save_path)}, commit=True)\n                logging.info(f\"Prediction grid saved to {grid_save_path} and logged.\")\n            else: logging.warning(\"No images collected for grid.\")\n        except Exception as e: logging.error(f\"Failed grid gen: {e}\", exc_info=True); plt.close()\n\n\n        # --- Save Final Best Model ---\n        if best_model_state:\n            save_path = os.path.join(fixed_args['output_dir'], fixed_args['model_save_name'])\n            os.makedirs(fixed_args['output_dir'], exist_ok=True)\n            try:\n                save_dict = {'epoch': best_epoch, 'model_state_dict': best_model_state, 'best_val_accuracy': best_val_acc, 'config': config}\n                torch.save(save_dict, save_path)\n                logging.info(f\"Best model state saved to {save_path}\")\n                model_artifact = wandb.Artifact(f\"best-model-{run.id}\", type=\"model\", description=f\"Best model from run {run.name}\", metadata=dict(config)) # Use dict(config)\n                model_artifact.add_file(save_path)\n                wandb.log_artifact(model_artifact)\n                logging.info(\"Best model logged as W&B artifact.\")\n            except Exception as e: logging.error(f\"Failed to save best model state: {e}\", exc_info=True)\n        else: logging.warning(\"No best model state found to save.\")\n\n    except Exception as e:\n         logging.error(f\"Unhandled error in train_and_evaluate: {e}\", exc_info=True)\n         if run and wandb.run: wandb.finish(exit_code=1)\n\n    finally:\n        # Final check to ensure W&B run finishes\n        if run and wandb.run is not None and wandb.run.id == run.id:\n             if hasattr(wandb.run, 'finished') and not wandb.run.finished:\n                 try: wandb.finish()\n                 except Exception as fe: logging.error(f\"Error finishing W&B run: {fe}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:18:44.729389Z","iopub.execute_input":"2025-04-19T20:18:44.729735Z","iopub.status.idle":"2025-04-19T20:18:44.764319Z","shell.execute_reply.started":"2025-04-19T20:18:44.729709Z","shell.execute_reply":"2025-04-19T20:18:44.763603Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # --- Define fixed arguments (not hyperparameters) ---\n    # Using defaults or could use a simple argparse for these if needed\n    fixed_args = {\n        \"data_dir\": DATA_DIR,\n        \"output_dir\": OUTPUT_DIR,\n        \"model_save_name\": MODEL_SAVE_NAME,\n        \"wandb_project\": WANDB_PROJECT_NAME,\n        \"wandb_entity\": WANDB_ENTITY,\n        \"seed\": SEED,\n        \"img_size\": IMG_SIZE,\n        \"num_workers\": NUM_WORKERS,\n        \"val_split\": VAL_SPLIT,\n        \"grid_rows\": GRID_ROWS,\n        \"grid_cols\": GRID_COLS,\n    }\n\n    # --- Basic Validation ---\n    if not os.path.isdir(fixed_args['data_dir']):\n        logging.error(f\"Data directory not found: {fixed_args['data_dir']}\")\n        sys.exit(1)\n\n    logging.info(f\"Starting run with BEST_CONFIG defined in script.\")\n    if 'BEST_CONFIG' not in globals():\n        logging.error(\"BEST_CONFIG dictionary not defined!\")\n        sys.exit(1)\n\n    # --- Run Training & Evaluation using BEST_CONFIG ---\n    train_and_evaluate(BEST_CONFIG, fixed_args) # Pass config and fixed args\n\n    print(\"--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:20:06.191249Z","iopub.execute_input":"2025-04-19T20:20:06.191533Z","iopub.status.idle":"2025-04-19T20:43:00.211308Z","shell.execute_reply.started":"2025-04-19T20:20:06.191513Z","shell.execute_reply":"2025-04-19T20:43:00.210469Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_202006-k1uyuyfg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP/runs/k1uyuyfg' target=\"_blank\">f64_k3_d256_bs32_mis_sam_lr5p0e04_do0p3_bnT_augT</a></strong> to <a href='https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP' target=\"_blank\">https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP/runs/k1uyuyfg' target=\"_blank\">https://wandb.ai/teja_sai-indian-institute-of-technology-madras/CNN_FROM_SCRATCH_SWEEP/runs/k1uyuyfg</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Ep 1 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 1 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 2 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 2 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 3 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 3 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 4 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 4 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 5 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 5 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 6 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 6 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 7 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 7 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 8 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 8 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 9 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 9 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 10 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 10 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 11 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 11 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 12 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 12 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 13 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 13 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 14 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 14 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 15 Train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Ep 15 Val :   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"--- Script Finished ---\n","output_type":"stream"}],"execution_count":13}]}